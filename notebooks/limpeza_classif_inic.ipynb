{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas e dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.5.0/pt_core_news_lg-3.5.0-py3-none-any.whl (568.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m602.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./venv/lib/python3.8/site-packages (from pt-core-news-lg==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.10.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (56.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import pt_core_news_lg\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from LeIA import SentimentIntensityAnalyzer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5955/4045662701.py:2: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, on_bad_lines='skip', sep=';', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/raw/ErikakHilton-tweets.csv'\n",
    "df = pd.read_csv(file_path, on_bad_lines='skip', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22505, 49)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar tweets?\n",
    "Pensei em filtrar os tweets, deixando somente os tweets direcionados a Erika, e para remover duplicatas.\n",
    "Nessa fase n√£o apliquei somente pra termos mais tweets para an√°lise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['user'] != \"https://twitter.com/ErikakHilton\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removendo \"n√£o\" das stopwords\n",
    "STOP_WORDS.remove('n√£o')\n",
    "\n",
    "# Instanciando spacy/pt_core_news_lg - corpus completo\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = df['rawContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900     @ErikakHilton Bolsonaro n√£o √© honesto coisa ne...\n",
       "3620     @ErikakHilton Faz isso n√£o, na √©poca que ele f...\n",
       "18066    @ErikakHilton @LulaOficial @jairbolsonaro O FA...\n",
       "19879    @ErikakHilton vamos ter representatividade no ...\n",
       "7694                                    @ErikakHilton Fake\n",
       "2672                         @ErikakHilton @BaixaEssaPorra\n",
       "13467                        @ErikakHilton PINTOU UM CLIMA\n",
       "21901    @ErikakHilton @visoesoniricas eu n aguento td ...\n",
       "18227             @ErikakHilton @camaradeputados Que √≥timo\n",
       "14976                               @ErikakHilton @S1n1nh4\n",
       "Name: rawContent, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover URL\n",
    "def remover_url(texto):\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover contas\n",
    "def remover_twiters(texto):\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remover pontua√ß√µes\n",
    "def remover_pontuacao(texto):\n",
    "    texto = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aplicando limpezas\n",
    "tweets = tweets.str.lower()\n",
    "tweets = tweets.apply(remover_url)\n",
    "tweets = tweets.apply(remover_twiters)\n",
    "tweets = tweets.apply(remover_pontuacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11821                                                     \n",
       "13847     o que sabemos  e sempre soubemos  voc√™ era um...\n",
       "7754                                          cuteeeeeeeee\n",
       "12198    levantamento divulgado hoje 1910 ouviu 2912 pe...\n",
       "21989                               lula presidente amanh√£\n",
       "17203                                              nunca\\n\n",
       "2649                                               1Ô∏è‚É£3Ô∏è‚É£ \n",
       "6790                                                      \n",
       "8916      eles est√£o desesperados\\neles est√£o desespera...\n",
       "13555                                   quem quer dinheiro\n",
       "Name: rawContent, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para remo√ß√£o de stopwords e digitos\n",
    "def limpar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_digit]\n",
    "    texto_limpo = \" \".join(tokens)\n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = tweets.apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativa de corrigir typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa biblioteca seria possivel corrigir erros de digita√ß√£o.<br>\n",
    "Em alguns tweets percebi que tinham palavras com letras repetidas ou contra√ß√µes e isso poderia dificutar a nossa an√°lise.<br>\n",
    "Mas n√£o consegui aplicar essa etapa da limpeza. Vou deixar marcado caso alguem queira tentar aplicar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando spellchecker\n",
    "# spell = SpellChecker(language='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para corre√ß√£o de grafia\n",
    "# def corretor_grafia(texto):\n",
    "#     tb = TextBlob(texto)\n",
    "#     corrected_words = []\n",
    "#     for word in tb.words:\n",
    "#         corrected_word = word.correct()\n",
    "#         corrected_words.append(corrected_word)\n",
    "#     return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets = df_tweets.apply(corretor_grafia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformando em dataframe\n",
    "df_tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identificando e excluindo linhas vazias. \n",
    "# Aqui n√£o foi poss√≠vel excluir nem identificar com fun√ß√µes NA, pq a linhas estava vazia.\n",
    "df_tweets = df_tweets[df_tweets['rawContent'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   op√ß√µes m√£e tias candidatas sa√≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sair√£o ‚ô°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>for√ßa fabio üíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ‚ù§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent\n",
       "0      longo dia recebendo den√∫ncias pessoas presas h...\n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è\n",
       "2                                        hahahhaha üíú üíú üíú\n",
       "3      longo dia recebendo den√∫ncias pessoas presas h...\n",
       "5                                                   ü´∂üèæü´∂üèæ\n",
       "...                                                  ...\n",
       "22500        casa vc   op√ß√µes m√£e tias candidatas sa√≠...\n",
       "22501                                           sair√£o ‚ô°\n",
       "22502  candidatos pra estadual federal respectivamente  \n",
       "22503                                      for√ßa fabio üíú\n",
       "22504                               ownnn conte comigo ‚ù§\n",
       "\n",
       "[21357 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para lemmatiza√ß√£o\n",
    "def lemmatiza(texto):\n",
    "    doc = nlp(texto)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        lemmatized_tokens.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['tweets_lemma'] = df_tweets['rawContent'].apply(lemmatiza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "      <td>longo dia receber den√∫ncia pessoa presas haver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "      <td>longo dia receber den√∫ncia pessoa presas haver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   op√ß√µes m√£e tias candidatas sa√≠...</td>\n",
       "      <td>casa vc    op√ß√£o m√£e tia candidato sair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sair√£o ‚ô°</td>\n",
       "      <td>sair ‚ô°</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "      <td>candidato pra estadual federal respectivamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>for√ßa fabio üíú</td>\n",
       "      <td>for√ßa Fabio üíú</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ‚ù§</td>\n",
       "      <td>ownnn contar comigo ‚ù§</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "0      longo dia recebendo den√∫ncias pessoas presas h...   \n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è   \n",
       "2                                        hahahhaha üíú üíú üíú   \n",
       "3      longo dia recebendo den√∫ncias pessoas presas h...   \n",
       "5                                                   ü´∂üèæü´∂üèæ   \n",
       "...                                                  ...   \n",
       "22500        casa vc   op√ß√µes m√£e tias candidatas sa√≠...   \n",
       "22501                                           sair√£o ‚ô°   \n",
       "22502  candidatos pra estadual federal respectivamente     \n",
       "22503                                      for√ßa fabio üíú   \n",
       "22504                               ownnn conte comigo ‚ù§   \n",
       "\n",
       "                                            tweets_lemma  \n",
       "0      longo dia receber den√∫ncia pessoa presas haver...  \n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è  \n",
       "2                                        hahahhaha üíú üíú üíú  \n",
       "3      longo dia receber den√∫ncia pessoa presas haver...  \n",
       "5                                                   ü´∂üèæü´∂üèæ  \n",
       "...                                                  ...  \n",
       "22500         casa vc    op√ß√£o m√£e tia candidato sair...  \n",
       "22501                                             sair ‚ô°  \n",
       "22502   candidato pra estadual federal respectivamente    \n",
       "22503                                      for√ßa Fabio üíú  \n",
       "22504                              ownnn contar comigo ‚ù§  \n",
       "\n",
       "[21357 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classifica√ß√£o de palavras\n",
    "def pos_tag(texto):\n",
    "    doc = nlp(texto)\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        pos_tags.append((token.text, token.pos_))\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['tweets_pos'] = df_tweets['rawContent'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "1        [(  , SPACE), (simplesmente, ADV), (‚ù§, NOUN), ...\n",
       "2        [(  , SPACE), (hahahhaha, NOUN), (üíú, PUNCT), (...\n",
       "3        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "5                              [(  , SPACE), (ü´∂üèæü´∂üèæ, NOUN)]\n",
       "                               ...                        \n",
       "22500    [(      , SPACE), (casa, NOUN), (vc, PROPN), (...\n",
       "22501         [(      , SPACE), (sair√£o, VERB), (‚ô°, NOUN)]\n",
       "22502    [(candidatos, NOUN), (pra, ADP), (estadual, NO...\n",
       "22503    [(   , SPACE), (for√ßa, NOUN), (fabio, PROPN), ...\n",
       "22504    [(   , SPACE), (ownnn, NOUN), (conte, VERB), (...\n",
       "Name: tweets_pos, Length: 21357, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweets_pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Completo\n",
    "Para facilitar e aplicar todas as transforma√ß√µes e limpezas foi proposto pipeline, por√©m ele ainda n√£o funciona.\n",
    "Caso queiram aplicar mandem bala. Em uma das conversas com ChatGPT uma hipotese levantada foi de que o pipeline n√£o suporta emojis e caracteres especial. Ent√£o precisariamos de uma limpeza maior. Ai entra outro ponto, se tirarmos todos esses caracteres podemos processar com mais eficacia, por√©m perderiamos informa√ß√µes no meio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        pos_tags.append(token.pos_)\n",
    "    return pos_tags\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        lemmatized_tokens.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    return texts.apply(lemmatize)\n",
    "\n",
    "def create_tfidf_matrix(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    # ('pos_tagging', FunctionTransformer(pos_tagging)),\n",
    "    ('lemmatization', FunctionTransformer(lemmatize_texts)),\n",
    "    ('tfidf', FunctionTransformer(create_tfidf_matrix)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessed_data = pipeline.fit_transform(df_tweets['rawContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tagged\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(pos_tagging)\n\u001b[0;32m----> 2\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tagged\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mlemmatize_text\u001b[49m)\n\u001b[1;32m      3\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(create_tfidf_matrix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatize_text' is not defined"
     ]
    }
   ],
   "source": [
    "df_tweets['pos_tagged'] = df_tweets['rawContent'].apply(pos_tagging)\n",
    "df_tweets['lemma'] = df_tweets['pos_tagged'].apply(lemmatize_text)\n",
    "df_tweets['tfidf_matrix'] = df_tweets['lemma'].apply(create_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos n√£o supervisionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-based Approach (CBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paim/projects/projeto-tera/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar e preprocessar\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar cluster para agrupar tweets similares\n",
    "k = 2  # numero de clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Rotular (n√£o entendi exatamente porque e como estipularam 0 para positivo)\n",
    "df_tweets['classif_cba'] = ['positive' if label == 0 else 'negative' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    20379\n",
       "negative      978\n",
       "Name: classif_cba, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_cba'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling-based Approach (TMBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizar e preprocessar\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar LDA para identificar topicos/temas\n",
    "n_topics = 2  # numero de topicos\n",
    "lda = LatentDirichletAllocation(n_components=n_topics)\n",
    "lda.fit(X)\n",
    "\n",
    "# Manualmente atribuir rotulos para cada topico\n",
    "topic_sentiments = ['positive', 'negative']\n",
    "topic_labels = [topic_sentiments[i] for i in lda.transform(X).argmax(axis=1)]\n",
    "\n",
    "# Rotular tweets baseado em topicos\n",
    "df_tweets['classif_tmba'] = topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    12969\n",
       "negative     8388\n",
       "Name: classif_tmba, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_tmba'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity Analysis (SIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon') # download do sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# N√£o √© adequado para o portugues, mas existem adapta√ß√µes como LeIA \n",
    "# https://github.com/rafjaa/LeIA\n",
    "\n",
    "sia = SentimentIntensityAnalyzer() # criar sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop pelos tweets e retornar score de sentimento\n",
    "sentiments = []\n",
    "\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    sentiment = sia.polarity_scores(tweet)\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['sentiment_sia'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}            18897\n",
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}             1447\n",
       "{'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}       33\n",
       "{'neg': 0.0, 'neu': 0.333, 'pos': 0.667, 'compound': 0.6124}        27\n",
       "{'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'compound': -0.7906}       25\n",
       "                                                                 ...  \n",
       "{'neg': 0.0, 'neu': 0.545, 'pos': 0.455, 'compound': 0.3612}         1\n",
       "{'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215}         1\n",
       "{'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.5106}         1\n",
       "{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'compound': 0.4939}         1\n",
       "{'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compound': 0.5106}           1\n",
       "Name: sentiment_sia, Length: 418, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['sentiment_sia'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeIA (L√©xico para Infer√™ncia Adaptada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LeIA import SentimentIntensityAnalyzer as LeIASentimentIntensityAnalyzer\n",
    "\n",
    "sia = LeIASentimentIntensityAnalyzer() # criar sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop pelos tweets e retornar score de sentimento\n",
    "sentiments_leia = []\n",
    "\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    sentiment = sia.polarity_scores(tweet)\n",
    "    sentiments_leia.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['sentiment_sia_leia'] = sentiments_leia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}              11014\n",
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}               1540\n",
       "{'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'compound': -0.296}         227\n",
       "{'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'compound': 0.5859}         196\n",
       "{'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.296}         155\n",
       "                                                                   ...  \n",
       "{'neg': 0.384, 'neu': 0.434, 'pos': 0.182, 'compound': -0.7906}        1\n",
       "{'neg': 0.25, 'neu': 0.53, 'pos': 0.22, 'compound': -0.1027}           1\n",
       "{'neg': 0.191, 'neu': 0.601, 'pos': 0.208, 'compound': 0.0772}         1\n",
       "{'neg': 0.231, 'neu': 0.769, 'pos': 0.0, 'compound': -0.765}           1\n",
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.0516}              1\n",
       "Name: sentiment_sia_leia, Length: 3364, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['sentiment_sia_leia'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_lemma</th>\n",
       "      <th>sentiment_sia_leia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>bolsonaro mentir pra agradecer algu√©m vacinar sentido agradecer luta Jo√£o doria \\n\\n  ser bolson...</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.783, 'pos': 0.135, 'compound': 0.3612}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>tarc√≠sio freitas gostar vender t√©cnico carregar mau fanatismo bolsonarista importar Paulo pensam...</td>\n",
       "      <td>{'neg': 0.154, 'neu': 0.752, 'pos': 0.094, 'compound': -0.1531}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>hoje sair not√≠cia posto ipiranga pensar mudar aumento sal√°rio m√≠nimo passar infla√ß√£o prever a...</td>\n",
       "      <td>{'neg': 0.124, 'neu': 0.876, 'pos': 0.0, 'compound': -0.34}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>passe livre algu√©m pagar conta adivinho    ü§°</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.25}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17338</th>\n",
       "      <td>kkkk</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17923</th>\n",
       "      <td>Lula vencer turno milh√£o voto estar forte apoio parte pa√≠s assista programa turno ir frente bras...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.789, 'pos': 0.132, 'compound': 0.25}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16338</th>\n",
       "      <td>falar voo n√£o pq tarc√≠si0 capaz cair ponte a√©reo \\n  haddadprontoprasp haddadgovernadorsp1Ô∏è‚É£3Ô∏è‚É£</td>\n",
       "      <td>{'neg': 0.159, 'neu': 0.652, 'pos': 0.188, 'compound': 0.1027}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>propor regi√£o üìç ribeir√£o \\n  vote    1Ô∏è‚É£3Ô∏è‚É£ ‚úÖ</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>Gataaaa</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>ir ganhar erika üôè üèº üôè üèº üôè üèº    querer prejudicar nordeste hoje gente resistir</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.5859}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              tweets_lemma  \\\n",
       "3572   bolsonaro mentir pra agradecer algu√©m vacinar sentido agradecer luta Jo√£o doria \\n\\n  ser bolson...   \n",
       "15349  tarc√≠sio freitas gostar vender t√©cnico carregar mau fanatismo bolsonarista importar Paulo pensam...   \n",
       "11498     hoje sair not√≠cia posto ipiranga pensar mudar aumento sal√°rio m√≠nimo passar infla√ß√£o prever a...   \n",
       "8912                                                          passe livre algu√©m pagar conta adivinho    ü§°   \n",
       "17338                                                                                                 kkkk   \n",
       "17923  Lula vencer turno milh√£o voto estar forte apoio parte pa√≠s assista programa turno ir frente bras...   \n",
       "16338      falar voo n√£o pq tarc√≠si0 capaz cair ponte a√©reo \\n  haddadprontoprasp haddadgovernadorsp1Ô∏è‚É£3Ô∏è‚É£   \n",
       "8256                                                         propor regi√£o üìç ribeir√£o \\n  vote    1Ô∏è‚É£3Ô∏è‚É£ ‚úÖ   \n",
       "19141                                                                                              Gataaaa   \n",
       "834                          ir ganhar erika üôè üèº üôè üèº üôè üèº    querer prejudicar nordeste hoje gente resistir   \n",
       "\n",
       "                                                    sentiment_sia_leia  \n",
       "3572    {'neg': 0.082, 'neu': 0.783, 'pos': 0.135, 'compound': 0.3612}  \n",
       "15349  {'neg': 0.154, 'neu': 0.752, 'pos': 0.094, 'compound': -0.1531}  \n",
       "11498      {'neg': 0.124, 'neu': 0.876, 'pos': 0.0, 'compound': -0.34}  \n",
       "8912        {'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.25}  \n",
       "17338            {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "17923     {'neg': 0.079, 'neu': 0.789, 'pos': 0.132, 'compound': 0.25}  \n",
       "16338   {'neg': 0.159, 'neu': 0.652, 'pos': 0.188, 'compound': 0.1027}  \n",
       "8256             {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "19141            {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "834       {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.5859}  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_tweets.sample(10)[['tweets_lemma', 'sentiment_sia_leia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21357, 6)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos supervisionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fun√ß√£o para classifica√ß√£o usando sentiment140\n",
    "def classif_sent140(tweet):\n",
    "    # N√£o √© preparado para portugues\n",
    "    analise = TextBlob(tweet)\n",
    "    if analise.sentiment.polarity > 0:\n",
    "        return 'positivo'\n",
    "    elif analise.sentiment.polarity == 0:\n",
    "        return 'neutro'\n",
    "    else:\n",
    "        return 'negativo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['classif_sent140'] = df_tweets['tweets_lemma'].apply(classif_sent140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutro      20191\n",
       "positivo      927\n",
       "negativo      239\n",
       "Name: classif_sent140, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_sent140'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Carregar modelo pr√© treinado BERTimbau\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#atribuir an√°lise ao CPU\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "# Fun√ß√£o para analise sentimento\n",
    "def predict_sentiment(tweet):\n",
    "    # Encode do tweet com tokenizer do BERT\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    # Input IDs and attention mask (n√£o entendi exatamente essa parte)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    # Fazer previs√£o com BERTimbau\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    # Rotulos de classifica√ß√£o (positivo ou negativo)\n",
    "    predicted_label = torch.argmax(outputs[0]).item()\n",
    "    # Returnar rotulos previstos\n",
    "    return \"positivo\" if predicted_label == 1 else \"negativo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lista vazia para armazenar classifica√ß√£o de cada tweet\n",
    "predicted_sentiments = []\n",
    "\n",
    "# Loop por cada tweet e previs√£o\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    predicted_sentiment = predict_sentiment(tweet)\n",
    "    predicted_sentiments.append(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets[\"classif_bertimbau\"] = predicted_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positivo    21169\n",
       "negativo      188\n",
       "Name: classif_bertimbau, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[\"classif_bertimbau\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'predicted_sentiment':'classif_bertimbau'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa linha abaixo eu quis criar uma coluna com o compound da classifica√ß√£o SIA, pois o resultado do modelo √© um dicion√°rio com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4004766246.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[98], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Nessa linha eu quis criar uma coluna com o compound da classifica√ß√£o SIA, pois o resultado do modelo √© um dicion√°rio com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "\n",
    "for result in df_tweets['sentiment_sia']:\n",
    "    compound = result['compound']\n",
    "    list.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['compound_sia'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'sentiment_sia':'classif_sia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slavando resultado\n",
    "df_tweets.to_csv('data/df_tweets_classif.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dataframe\n",
    "df_novo = pd.read_csv('data/df_tweets_classif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "      <th>tweets_pos</th>\n",
       "      <th>classif_cba</th>\n",
       "      <th>classif_tmba</th>\n",
       "      <th>classif_sia</th>\n",
       "      <th>classif_sent140</th>\n",
       "      <th>classif_bertimbau</th>\n",
       "      <th>compound_sia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "      <td>longo dia receber den√∫ncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "      <td>simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è</td>\n",
       "      <td>[('  ', 'SPACE'), ('simplesmente', 'ADV'), ('‚ù§...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "      <td>hahahhaha üíú üíú üíú</td>\n",
       "      <td>[('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('üíú',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo den√∫ncias pessoas presas h...</td>\n",
       "      <td>longo dia receber den√∫ncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "      <td>ü´∂üèæü´∂üèæ</td>\n",
       "      <td>[('  ', 'SPACE'), ('\\U0001faf6üèæ\\U0001faf6üèæ', '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   op√ß√µes m√£e tias candidatas sa√≠...</td>\n",
       "      <td>casa vc    op√ß√£o m√£e tia candidato sair...</td>\n",
       "      <td>[('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sair√£o ‚ô°</td>\n",
       "      <td>sair ‚ô°</td>\n",
       "      <td>[('      ', 'SPACE'), ('sair√£o', 'VERB'), ('‚ô°'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "      <td>candidato pra estadual federal respectivamente</td>\n",
       "      <td>[('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>for√ßa fabio üíú</td>\n",
       "      <td>for√ßa Fabio üíú</td>\n",
       "      <td>[('   ', 'SPACE'), ('for√ßa', 'NOUN'), ('fabio'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ‚ù§</td>\n",
       "      <td>ownnn contar comigo ‚ù§</td>\n",
       "      <td>[('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "0      longo dia recebendo den√∫ncias pessoas presas h...   \n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è   \n",
       "2                                        hahahhaha üíú üíú üíú   \n",
       "3      longo dia recebendo den√∫ncias pessoas presas h...   \n",
       "5                                                   ü´∂üèæü´∂üèæ   \n",
       "...                                                  ...   \n",
       "22500        casa vc   op√ß√µes m√£e tias candidatas sa√≠...   \n",
       "22501                                           sair√£o ‚ô°   \n",
       "22502  candidatos pra estadual federal respectivamente     \n",
       "22503                                      for√ßa fabio üíú   \n",
       "22504                               ownnn conte comigo ‚ù§   \n",
       "\n",
       "                                            tweets_lemma  \\\n",
       "0      longo dia receber den√∫ncia pessoa presas haver...   \n",
       "1                               simplesmente ‚ù§ Ô∏è ‚ù§ Ô∏è ‚ù§ Ô∏è   \n",
       "2                                        hahahhaha üíú üíú üíú   \n",
       "3      longo dia receber den√∫ncia pessoa presas haver...   \n",
       "5                                                   ü´∂üèæü´∂üèæ   \n",
       "...                                                  ...   \n",
       "22500         casa vc    op√ß√£o m√£e tia candidato sair...   \n",
       "22501                                             sair ‚ô°   \n",
       "22502   candidato pra estadual federal respectivamente     \n",
       "22503                                      for√ßa Fabio üíú   \n",
       "22504                              ownnn contar comigo ‚ù§   \n",
       "\n",
       "                                              tweets_pos classif_cba  \\\n",
       "0      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "1      [('  ', 'SPACE'), ('simplesmente', 'ADV'), ('‚ù§...    positive   \n",
       "2      [('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('üíú',...    positive   \n",
       "3      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "5      [('  ', 'SPACE'), ('\\U0001faf6üèæ\\U0001faf6üèæ', '...    positive   \n",
       "...                                                  ...         ...   \n",
       "22500  [('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...    positive   \n",
       "22501  [('      ', 'SPACE'), ('sair√£o', 'VERB'), ('‚ô°'...    positive   \n",
       "22502  [('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...    positive   \n",
       "22503  [('   ', 'SPACE'), ('for√ßa', 'NOUN'), ('fabio'...    positive   \n",
       "22504  [('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...    positive   \n",
       "\n",
       "      classif_tmba                                        classif_sia  \\\n",
       "0         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "2         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "3         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "5         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "...            ...                                                ...   \n",
       "22500     positive  {'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...   \n",
       "22501     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22502     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22503     negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22504     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "      classif_sent140 classif_bertimbau  compound_sia  \n",
       "0              neutro          negative        0.0000  \n",
       "1              neutro          negative        0.0000  \n",
       "2              neutro          negative        0.0000  \n",
       "3              neutro          negative        0.0000  \n",
       "5              neutro          negative        0.0000  \n",
       "...               ...               ...           ...  \n",
       "22500          neutro          negative        0.5106  \n",
       "22501          neutro          negative        0.0000  \n",
       "22502          neutro          negative        0.0000  \n",
       "22503          neutro          negative        0.0000  \n",
       "22504          neutro          negative        0.0000  \n",
       "\n",
       "[21357 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2852de53f51e57db0fb3c92b7d03cd28fd1e6acc643c7bf23f4addc7db48be7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
