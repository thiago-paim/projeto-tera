{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas e dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.5.0/pt_core_news_lg-3.5.0-py3-none-any.whl (568.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m602.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in ./venv/lib/python3.8/site-packages (from pt-core-news-lg==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.10.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (56.0.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.23.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in ./venv/lib/python3.8/site-packages (from spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./venv/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./venv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./venv/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./venv/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.8/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->pt-core-news-lg==3.5.0) (2.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import nltk\n",
    "import pt_core_news_lg\n",
    "from textblob import TextBlob\n",
    "from spellchecker import SpellChecker\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from LeIA import SentimentIntensityAnalyzer\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5955/4045662701.py:2: DtypeWarning: Columns (25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, on_bad_lines='skip', sep=';', encoding='utf-8')\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/raw/ErikakHilton-tweets.csv'\n",
    "df = pd.read_csv(file_path, on_bad_lines='skip', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22505, 49)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar tweets?\n",
    "Pensei em filtrar os tweets, deixando somente os tweets direcionados a Erika, e para remover duplicatas.\n",
    "Nessa fase não apliquei somente pra termos mais tweets para análise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['user'] != \"https://twitter.com/ErikakHilton\"].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Limpeza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removendo \"não\" das stopwords\n",
    "STOP_WORDS.remove('não')\n",
    "\n",
    "# Instanciando spacy/pt_core_news_lg - corpus completo\n",
    "nlp = spacy.load('pt_core_news_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = df['rawContent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9900     @ErikakHilton Bolsonaro não é honesto coisa ne...\n",
       "3620     @ErikakHilton Faz isso não, na época que ele f...\n",
       "18066    @ErikakHilton @LulaOficial @jairbolsonaro O FA...\n",
       "19879    @ErikakHilton vamos ter representatividade no ...\n",
       "7694                                    @ErikakHilton Fake\n",
       "2672                         @ErikakHilton @BaixaEssaPorra\n",
       "13467                        @ErikakHilton PINTOU UM CLIMA\n",
       "21901    @ErikakHilton @visoesoniricas eu n aguento td ...\n",
       "18227             @ErikakHilton @camaradeputados Que ótimo\n",
       "14976                               @ErikakHilton @S1n1nh4\n",
       "Name: rawContent, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para remover URL\n",
    "def remover_url(texto):\n",
    "    texto = re.sub(r'http\\S+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para remover contas\n",
    "def remover_twiters(texto):\n",
    "    texto = re.sub(r'@\\w+', '', texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para remover pontuações\n",
    "def remover_pontuacao(texto):\n",
    "    texto = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aplicando limpezas\n",
    "tweets = tweets.str.lower()\n",
    "tweets = tweets.apply(remover_url)\n",
    "tweets = tweets.apply(remover_twiters)\n",
    "tweets = tweets.apply(remover_pontuacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11821                                                     \n",
       "13847     o que sabemos  e sempre soubemos  você era um...\n",
       "7754                                          cuteeeeeeeee\n",
       "12198    levantamento divulgado hoje 1910 ouviu 2912 pe...\n",
       "21989                               lula presidente amanhã\n",
       "17203                                              nunca\\n\n",
       "2649                                               1️⃣3️⃣ \n",
       "6790                                                      \n",
       "8916      eles estão desesperados\\neles estão desespera...\n",
       "13555                                   quem quer dinheiro\n",
       "Name: rawContent, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para remoção de stopwords e digitos\n",
    "def limpar_texto(texto):\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.text for token in doc if not token.is_stop and not token.is_digit]\n",
    "    texto_limpo = \" \".join(tokens)\n",
    "    return texto_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets = tweets.apply(limpar_texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tentativa de corrigir typos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com essa biblioteca seria possivel corrigir erros de digitação.<br>\n",
    "Em alguns tweets percebi que tinham palavras com letras repetidas ou contrações e isso poderia dificutar a nossa análise.<br>\n",
    "Mas não consegui aplicar essa etapa da limpeza. Vou deixar marcado caso alguem queira tentar aplicar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando spellchecker\n",
    "# spell = SpellChecker(language='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para correção de grafia\n",
    "# def corretor_grafia(texto):\n",
    "#     tb = TextBlob(texto)\n",
    "#     corrected_words = []\n",
    "#     for word in tb.words:\n",
    "#         corrected_word = word.correct()\n",
    "#         corrected_words.append(corrected_word)\n",
    "#     return ' '.join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tweets = df_tweets.apply(corretor_grafia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transformando em dataframe\n",
    "df_tweets = tweets.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identificando e excluindo linhas vazias. \n",
    "# Aqui não foi possível excluir nem identificar com funções NA, pq a linhas estava vazia.\n",
    "df_tweets = df_tweets[df_tweets['rawContent'] != \" \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   opções mãe tias candidatas saí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sairão ♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>força fabio 💜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ❤</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent\n",
       "0      longo dia recebendo denúncias pessoas presas h...\n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️\n",
       "2                                        hahahhaha 💜 💜 💜\n",
       "3      longo dia recebendo denúncias pessoas presas h...\n",
       "5                                                   🫶🏾🫶🏾\n",
       "...                                                  ...\n",
       "22500        casa vc   opções mãe tias candidatas saí...\n",
       "22501                                           sairão ♡\n",
       "22502  candidatos pra estadual federal respectivamente  \n",
       "22503                                      força fabio 💜\n",
       "22504                               ownnn conte comigo ❤\n",
       "\n",
       "[21357 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para lemmatização\n",
    "def lemmatiza(texto):\n",
    "    doc = nlp(texto)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        lemmatized_tokens.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['tweets_lemma'] = df_tweets['rawContent'].apply(lemmatiza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "      <td>longo dia receber denúncia pessoa presas haver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "      <td>longo dia receber denúncia pessoa presas haver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   opções mãe tias candidatas saí...</td>\n",
       "      <td>casa vc    opção mãe tia candidato sair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sairão ♡</td>\n",
       "      <td>sair ♡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "      <td>candidato pra estadual federal respectivamente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>força fabio 💜</td>\n",
       "      <td>força Fabio 💜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ❤</td>\n",
       "      <td>ownnn contar comigo ❤</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "0      longo dia recebendo denúncias pessoas presas h...   \n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️   \n",
       "2                                        hahahhaha 💜 💜 💜   \n",
       "3      longo dia recebendo denúncias pessoas presas h...   \n",
       "5                                                   🫶🏾🫶🏾   \n",
       "...                                                  ...   \n",
       "22500        casa vc   opções mãe tias candidatas saí...   \n",
       "22501                                           sairão ♡   \n",
       "22502  candidatos pra estadual federal respectivamente     \n",
       "22503                                      força fabio 💜   \n",
       "22504                               ownnn conte comigo ❤   \n",
       "\n",
       "                                            tweets_lemma  \n",
       "0      longo dia receber denúncia pessoa presas haver...  \n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️  \n",
       "2                                        hahahhaha 💜 💜 💜  \n",
       "3      longo dia receber denúncia pessoa presas haver...  \n",
       "5                                                   🫶🏾🫶🏾  \n",
       "...                                                  ...  \n",
       "22500         casa vc    opção mãe tia candidato sair...  \n",
       "22501                                             sair ♡  \n",
       "22502   candidato pra estadual federal respectivamente    \n",
       "22503                                      força Fabio 💜  \n",
       "22504                              ownnn contar comigo ❤  \n",
       "\n",
       "[21357 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para classificação de palavras\n",
    "def pos_tag(texto):\n",
    "    doc = nlp(texto)\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        pos_tags.append((token.text, token.pos_))\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['tweets_pos'] = df_tweets['rawContent'].apply(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "1        [(  , SPACE), (simplesmente, ADV), (❤, NOUN), ...\n",
       "2        [(  , SPACE), (hahahhaha, NOUN), (💜, PUNCT), (...\n",
       "3        [(longo, ADJ), (dia, NOUN), (recebendo, VERB),...\n",
       "5                              [(  , SPACE), (🫶🏾🫶🏾, NOUN)]\n",
       "                               ...                        \n",
       "22500    [(      , SPACE), (casa, NOUN), (vc, PROPN), (...\n",
       "22501         [(      , SPACE), (sairão, VERB), (♡, NOUN)]\n",
       "22502    [(candidatos, NOUN), (pra, ADP), (estadual, NO...\n",
       "22503    [(   , SPACE), (força, NOUN), (fabio, PROPN), ...\n",
       "22504    [(   , SPACE), (ownnn, NOUN), (conte, VERB), (...\n",
       "Name: tweets_pos, Length: 21357, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweets_pos']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Completo\n",
    "Para facilitar e aplicar todas as transformações e limpezas foi proposto pipeline, porém ele ainda não funciona.\n",
    "Caso queiram aplicar mandem bala. Em uma das conversas com ChatGPT uma hipotese levantada foi de que o pipeline não suporta emojis e caracteres especial. Então precisariamos de uma limpeza maior. Ai entra outro ponto, se tirarmos todos esses caracteres podemos processar com mais eficacia, porém perderiamos informações no meio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pos_tagging(text):\n",
    "    doc = nlp(text)\n",
    "    pos_tags = []\n",
    "    for token in doc:\n",
    "        pos_tags.append(token.pos_)\n",
    "    return pos_tags\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = []\n",
    "    for token in doc:\n",
    "        lemmatized_tokens.append(token.lemma_)\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "def lemmatize_texts(texts):\n",
    "    return texts.apply(lemmatize)\n",
    "\n",
    "def create_tfidf_matrix(texts):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
    "    return tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    # ('pos_tagging', FunctionTransformer(pos_tagging)),\n",
    "    ('lemmatization', FunctionTransformer(lemmatize_texts)),\n",
    "    ('tfidf', FunctionTransformer(create_tfidf_matrix)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessed_data = pipeline.fit_transform(df_tweets['rawContent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tagged\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawContent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(pos_tagging)\n\u001b[0;32m----> 2\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos_tagged\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[43mlemmatize_text\u001b[49m)\n\u001b[1;32m      3\u001b[0m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtfidf_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_tweets[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemma\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(create_tfidf_matrix)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lemmatize_text' is not defined"
     ]
    }
   ],
   "source": [
    "df_tweets['pos_tagged'] = df_tweets['rawContent'].apply(pos_tagging)\n",
    "df_tweets['lemma'] = df_tweets['pos_tagged'].apply(lemmatize_text)\n",
    "df_tweets['tfidf_matrix'] = df_tweets['lemma'].apply(create_tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos não supervisionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering-based Approach (CBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paim/projects/projeto-tera/venv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar e preprocessar\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar cluster para agrupar tweets similares\n",
    "k = 2  # numero de clusters\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "labels = kmeans.fit_predict(X)\n",
    "\n",
    "# Rotular (não entendi exatamente porque e como estipularam 0 para positivo)\n",
    "df_tweets['classif_cba'] = ['positive' if label == 0 else 'negative' for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    20379\n",
       "negative      978\n",
       "Name: classif_cba, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_cba'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling-based Approach (TMBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizar e preprocessar\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(df_tweets['tweets_lemma'])\n",
    "\n",
    "# Treinar LDA para identificar topicos/temas\n",
    "n_topics = 2  # numero de topicos\n",
    "lda = LatentDirichletAllocation(n_components=n_topics)\n",
    "lda.fit(X)\n",
    "\n",
    "# Manualmente atribuir rotulos para cada topico\n",
    "topic_sentiments = ['positive', 'negative']\n",
    "topic_labels = [topic_sentiments[i] for i in lda.transform(X).argmax(axis=1)]\n",
    "\n",
    "# Rotular tweets baseado em topicos\n",
    "df_tweets['classif_tmba'] = topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    12969\n",
       "negative     8388\n",
       "Name: classif_tmba, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_tmba'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Intensity Analysis (SIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon') # download do sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "# Não é adequado para o portugues, mas existem adaptações como LeIA \n",
    "# https://github.com/rafjaa/LeIA\n",
    "\n",
    "sia = SentimentIntensityAnalyzer() # criar sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop pelos tweets e retornar score de sentimento\n",
    "sentiments = []\n",
    "\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    sentiment = sia.polarity_scores(tweet)\n",
    "    sentiments.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['sentiment_sia'] = sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}            18897\n",
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}             1447\n",
       "{'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}       33\n",
       "{'neg': 0.0, 'neu': 0.333, 'pos': 0.667, 'compound': 0.6124}        27\n",
       "{'neg': 0.241, 'neu': 0.759, 'pos': 0.0, 'compound': -0.7906}       25\n",
       "                                                                 ...  \n",
       "{'neg': 0.0, 'neu': 0.545, 'pos': 0.455, 'compound': 0.3612}         1\n",
       "{'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215}         1\n",
       "{'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.5106}         1\n",
       "{'neg': 0.0, 'neu': 0.819, 'pos': 0.181, 'compound': 0.4939}         1\n",
       "{'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compound': 0.5106}           1\n",
       "Name: sentiment_sia, Length: 418, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['sentiment_sia'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeIA (Léxico para Inferência Adaptada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from LeIA import SentimentIntensityAnalyzer as LeIASentimentIntensityAnalyzer\n",
    "\n",
    "sia = LeIASentimentIntensityAnalyzer() # criar sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loop pelos tweets e retornar score de sentimento\n",
    "sentiments_leia = []\n",
    "\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    sentiment = sia.polarity_scores(tweet)\n",
    "    sentiments_leia.append(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['sentiment_sia_leia'] = sentiments_leia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}              11014\n",
       "{'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}               1540\n",
       "{'neg': 0.423, 'neu': 0.577, 'pos': 0.0, 'compound': -0.296}         227\n",
       "{'neg': 0.0, 'neu': 0.345, 'pos': 0.655, 'compound': 0.5859}         196\n",
       "{'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.296}         155\n",
       "                                                                   ...  \n",
       "{'neg': 0.384, 'neu': 0.434, 'pos': 0.182, 'compound': -0.7906}        1\n",
       "{'neg': 0.25, 'neu': 0.53, 'pos': 0.22, 'compound': -0.1027}           1\n",
       "{'neg': 0.191, 'neu': 0.601, 'pos': 0.208, 'compound': 0.0772}         1\n",
       "{'neg': 0.231, 'neu': 0.769, 'pos': 0.0, 'compound': -0.765}           1\n",
       "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.0516}              1\n",
       "Name: sentiment_sia_leia, Length: 3364, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['sentiment_sia_leia'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets_lemma</th>\n",
       "      <th>sentiment_sia_leia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3572</th>\n",
       "      <td>bolsonaro mentir pra agradecer alguém vacinar sentido agradecer luta João doria \\n\\n  ser bolson...</td>\n",
       "      <td>{'neg': 0.082, 'neu': 0.783, 'pos': 0.135, 'compound': 0.3612}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15349</th>\n",
       "      <td>tarcísio freitas gostar vender técnico carregar mau fanatismo bolsonarista importar Paulo pensam...</td>\n",
       "      <td>{'neg': 0.154, 'neu': 0.752, 'pos': 0.094, 'compound': -0.1531}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11498</th>\n",
       "      <td>hoje sair notícia posto ipiranga pensar mudar aumento salário mínimo passar inflação prever a...</td>\n",
       "      <td>{'neg': 0.124, 'neu': 0.876, 'pos': 0.0, 'compound': -0.34}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8912</th>\n",
       "      <td>passe livre alguém pagar conta adivinho    🤡</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.25}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17338</th>\n",
       "      <td>kkkk</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17923</th>\n",
       "      <td>Lula vencer turno milhão voto estar forte apoio parte país assista programa turno ir frente bras...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.789, 'pos': 0.132, 'compound': 0.25}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16338</th>\n",
       "      <td>falar voo não pq tarcísi0 capaz cair ponte aéreo \\n  haddadprontoprasp haddadgovernadorsp1️⃣3️⃣</td>\n",
       "      <td>{'neg': 0.159, 'neu': 0.652, 'pos': 0.188, 'compound': 0.1027}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>propor região 📍 ribeirão \\n  vote    1️⃣3️⃣ ✅</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>Gataaaa</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>ir ganhar erika 🙏 🏼 🙏 🏼 🙏 🏼    querer prejudicar nordeste hoje gente resistir</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.5859}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              tweets_lemma  \\\n",
       "3572   bolsonaro mentir pra agradecer alguém vacinar sentido agradecer luta João doria \\n\\n  ser bolson...   \n",
       "15349  tarcísio freitas gostar vender técnico carregar mau fanatismo bolsonarista importar Paulo pensam...   \n",
       "11498     hoje sair notícia posto ipiranga pensar mudar aumento salário mínimo passar inflação prever a...   \n",
       "8912                                                          passe livre alguém pagar conta adivinho    🤡   \n",
       "17338                                                                                                 kkkk   \n",
       "17923  Lula vencer turno milhão voto estar forte apoio parte país assista programa turno ir frente bras...   \n",
       "16338      falar voo não pq tarcísi0 capaz cair ponte aéreo \\n  haddadprontoprasp haddadgovernadorsp1️⃣3️⃣   \n",
       "8256                                                         propor região 📍 ribeirão \\n  vote    1️⃣3️⃣ ✅   \n",
       "19141                                                                                              Gataaaa   \n",
       "834                          ir ganhar erika 🙏 🏼 🙏 🏼 🙏 🏼    querer prejudicar nordeste hoje gente resistir   \n",
       "\n",
       "                                                    sentiment_sia_leia  \n",
       "3572    {'neg': 0.082, 'neu': 0.783, 'pos': 0.135, 'compound': 0.3612}  \n",
       "15349  {'neg': 0.154, 'neu': 0.752, 'pos': 0.094, 'compound': -0.1531}  \n",
       "11498      {'neg': 0.124, 'neu': 0.876, 'pos': 0.0, 'compound': -0.34}  \n",
       "8912        {'neg': 0.0, 'neu': 0.571, 'pos': 0.429, 'compound': 0.25}  \n",
       "17338            {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "17923     {'neg': 0.079, 'neu': 0.789, 'pos': 0.132, 'compound': 0.25}  \n",
       "16338   {'neg': 0.159, 'neu': 0.652, 'pos': 0.188, 'compound': 0.1027}  \n",
       "8256             {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "19141            {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}  \n",
       "834       {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.5859}  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "df_tweets.sample(10)[['tweets_lemma', 'sentiment_sia_leia']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21357, 6)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos supervisionados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função para classificação usando sentiment140\n",
    "def classif_sent140(tweet):\n",
    "    # Não é preparado para portugues\n",
    "    analise = TextBlob(tweet)\n",
    "    if analise.sentiment.polarity > 0:\n",
    "        return 'positivo'\n",
    "    elif analise.sentiment.polarity == 0:\n",
    "        return 'neutro'\n",
    "    else:\n",
    "        return 'negativo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets['classif_sent140'] = df_tweets['tweets_lemma'].apply(classif_sent140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "neutro      20191\n",
       "positivo      927\n",
       "negativo      239\n",
       "Name: classif_sent140, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['classif_sent140'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Carregar tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmind/bert-base-portuguese-cased\")\n",
    "\n",
    "# Carregar modelo pré treinado BERTimbau\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"neuralmind/bert-base-portuguese-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#atribuir análise ao CPU\n",
    "device = torch.device('cpu')\n",
    "# device = torch.device('cuda')\n",
    "\n",
    "# Função para analise sentimento\n",
    "def predict_sentiment(tweet):\n",
    "    # Encode do tweet com tokenizer do BERT\n",
    "    inputs = tokenizer.encode_plus(tweet, add_special_tokens=True, return_tensors=\"pt\")\n",
    "    # Input IDs and attention mask (não entendi exatamente essa parte)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    # Fazer previsão com BERTimbau\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    # Rotulos de classificação (positivo ou negativo)\n",
    "    predicted_label = torch.argmax(outputs[0]).item()\n",
    "    # Returnar rotulos previstos\n",
    "    return \"positivo\" if predicted_label == 1 else \"negativo\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lista vazia para armazenar classificação de cada tweet\n",
    "predicted_sentiments = []\n",
    "\n",
    "# Loop por cada tweet e previsão\n",
    "for tweet in df_tweets['tweets_lemma']:\n",
    "    predicted_sentiment = predict_sentiment(tweet)\n",
    "    predicted_sentiments.append(predicted_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets[\"classif_bertimbau\"] = predicted_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positivo    21169\n",
       "negativo      188\n",
       "Name: classif_bertimbau, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets[\"classif_bertimbau\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'predicted_sentiment':'classif_bertimbau'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa linha abaixo eu quis criar uma coluna com o compound da classificação SIA, pois o resultado do modelo é um dicionário com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4004766246.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[98], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    Nessa linha eu quis criar uma coluna com o compound da classificação SIA, pois o resultado do modelo é um dicionário com os valores que o modelo calculou para cada sentimento(neutro, positivo e negativo, sendo o compound uma metrica relativa aos 3 sentimentos presentes no texto)\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "list = []\n",
    "\n",
    "for result in df_tweets['sentiment_sia']:\n",
    "    compound = result['compound']\n",
    "    list.append(compound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['compound_sia'] = list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.rename(columns={'sentiment_sia':'classif_sia'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slavando resultado\n",
    "df_tweets.to_csv('data/df_tweets_classif.csv', index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dataframe\n",
    "df_novo = pd.read_csv('data/df_tweets_classif.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rawContent</th>\n",
       "      <th>tweets_lemma</th>\n",
       "      <th>tweets_pos</th>\n",
       "      <th>classif_cba</th>\n",
       "      <th>classif_tmba</th>\n",
       "      <th>classif_sia</th>\n",
       "      <th>classif_sent140</th>\n",
       "      <th>classif_bertimbau</th>\n",
       "      <th>compound_sia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "      <td>longo dia receber denúncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "      <td>simplesmente ❤ ️ ❤ ️ ❤ ️</td>\n",
       "      <td>[('  ', 'SPACE'), ('simplesmente', 'ADV'), ('❤...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "      <td>hahahhaha 💜 💜 💜</td>\n",
       "      <td>[('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('💜',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>longo dia recebendo denúncias pessoas presas h...</td>\n",
       "      <td>longo dia receber denúncia pessoa presas haver...</td>\n",
       "      <td>[('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "      <td>🫶🏾🫶🏾</td>\n",
       "      <td>[('  ', 'SPACE'), ('\\U0001faf6🏾\\U0001faf6🏾', '...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22500</th>\n",
       "      <td>casa vc   opções mãe tias candidatas saí...</td>\n",
       "      <td>casa vc    opção mãe tia candidato sair...</td>\n",
       "      <td>[('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.5106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22501</th>\n",
       "      <td>sairão ♡</td>\n",
       "      <td>sair ♡</td>\n",
       "      <td>[('      ', 'SPACE'), ('sairão', 'VERB'), ('♡'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22502</th>\n",
       "      <td>candidatos pra estadual federal respectivamente</td>\n",
       "      <td>candidato pra estadual federal respectivamente</td>\n",
       "      <td>[('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22503</th>\n",
       "      <td>força fabio 💜</td>\n",
       "      <td>força Fabio 💜</td>\n",
       "      <td>[('   ', 'SPACE'), ('força', 'NOUN'), ('fabio'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22504</th>\n",
       "      <td>ownnn conte comigo ❤</td>\n",
       "      <td>ownnn contar comigo ❤</td>\n",
       "      <td>[('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>neutro</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21357 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              rawContent  \\\n",
       "0      longo dia recebendo denúncias pessoas presas h...   \n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️   \n",
       "2                                        hahahhaha 💜 💜 💜   \n",
       "3      longo dia recebendo denúncias pessoas presas h...   \n",
       "5                                                   🫶🏾🫶🏾   \n",
       "...                                                  ...   \n",
       "22500        casa vc   opções mãe tias candidatas saí...   \n",
       "22501                                           sairão ♡   \n",
       "22502  candidatos pra estadual federal respectivamente     \n",
       "22503                                      força fabio 💜   \n",
       "22504                               ownnn conte comigo ❤   \n",
       "\n",
       "                                            tweets_lemma  \\\n",
       "0      longo dia receber denúncia pessoa presas haver...   \n",
       "1                               simplesmente ❤ ️ ❤ ️ ❤ ️   \n",
       "2                                        hahahhaha 💜 💜 💜   \n",
       "3      longo dia receber denúncia pessoa presas haver...   \n",
       "5                                                   🫶🏾🫶🏾   \n",
       "...                                                  ...   \n",
       "22500         casa vc    opção mãe tia candidato sair...   \n",
       "22501                                             sair ♡   \n",
       "22502   candidato pra estadual federal respectivamente     \n",
       "22503                                      força Fabio 💜   \n",
       "22504                              ownnn contar comigo ❤   \n",
       "\n",
       "                                              tweets_pos classif_cba  \\\n",
       "0      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "1      [('  ', 'SPACE'), ('simplesmente', 'ADV'), ('❤...    positive   \n",
       "2      [('  ', 'SPACE'), ('hahahhaha', 'NOUN'), ('💜',...    positive   \n",
       "3      [('longo', 'ADJ'), ('dia', 'NOUN'), ('recebend...    positive   \n",
       "5      [('  ', 'SPACE'), ('\\U0001faf6🏾\\U0001faf6🏾', '...    positive   \n",
       "...                                                  ...         ...   \n",
       "22500  [('      ', 'SPACE'), ('casa', 'NOUN'), ('vc',...    positive   \n",
       "22501  [('      ', 'SPACE'), ('sairão', 'VERB'), ('♡'...    positive   \n",
       "22502  [('candidatos', 'NOUN'), ('pra', 'ADP'), ('est...    positive   \n",
       "22503  [('   ', 'SPACE'), ('força', 'NOUN'), ('fabio'...    positive   \n",
       "22504  [('   ', 'SPACE'), ('ownnn', 'NOUN'), ('conte'...    positive   \n",
       "\n",
       "      classif_tmba                                        classif_sia  \\\n",
       "0         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "1         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "2         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "3         negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "5         positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "...            ...                                                ...   \n",
       "22500     positive  {'neg': 0.0, 'neu': 0.68, 'pos': 0.32, 'compou...   \n",
       "22501     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22502     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22503     negative  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "22504     positive  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...   \n",
       "\n",
       "      classif_sent140 classif_bertimbau  compound_sia  \n",
       "0              neutro          negative        0.0000  \n",
       "1              neutro          negative        0.0000  \n",
       "2              neutro          negative        0.0000  \n",
       "3              neutro          negative        0.0000  \n",
       "5              neutro          negative        0.0000  \n",
       "...               ...               ...           ...  \n",
       "22500          neutro          negative        0.5106  \n",
       "22501          neutro          negative        0.0000  \n",
       "22502          neutro          negative        0.0000  \n",
       "22503          neutro          negative        0.0000  \n",
       "22504          neutro          negative        0.0000  \n",
       "\n",
       "[21357 rows x 9 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_novo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2852de53f51e57db0fb3c92b7d03cd28fd1e6acc643c7bf23f4addc7db48be7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
